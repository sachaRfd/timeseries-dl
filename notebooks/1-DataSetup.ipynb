{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samples': tensor([[[ 41.8000,  55.2000,  -8.6000,  ...,  16.9000,  42.2000,  13.0000],\n",
       "          [ 44.8000,  53.8000,  -3.6000,  ...,  24.5000,  35.0000,  26.6000],\n",
       "          [ 47.1000,  59.9000,  14.4000,  ...,  24.5000,  41.7000,  52.5000],\n",
       "          ...,\n",
       "          [ 69.8000,  17.5000,  23.3000,  ...,  51.9000,  51.5000,  -3.5000],\n",
       "          [ 72.6000,  28.0000,  35.9000,  ...,  59.6000,  58.5000,  -3.2000],\n",
       "          [ 76.1000,  12.1000,  23.2000,  ...,  57.3000,  46.9000,  -2.6000]],\n",
       " \n",
       "         [[132.0000,  99.7000,  18.7000,  ..., -11.2000, -21.7000, -41.3000],\n",
       "          [137.0000,  95.8000,   0.8000,  ..., -11.6000, -21.1000, -40.2000],\n",
       "          [148.0000,  99.5000,  20.0000,  ..., -13.8000, -23.6000, -33.3000],\n",
       "          ...,\n",
       "          [ 78.1000, -39.9000,   0.9000,  ..., -31.9000, -57.8000, -51.1000],\n",
       "          [ 90.5000, -10.2000,  31.6000,  ..., -19.9000, -44.6000, -45.4000],\n",
       "          [ 85.7000, -37.4000,   7.3000,  ..., -25.7000, -45.6000, -42.5000]],\n",
       " \n",
       "         [[ 69.4000,  67.8000,   9.9000,  ...,  20.9000,  14.1000,   3.6000],\n",
       "          [ 57.0000,  49.0000, -12.4000,  ...,  20.5000,   7.9000,  11.0000],\n",
       "          [ 76.4000,  62.1000,  18.7000,  ...,  30.1000,   6.3000,  14.0000],\n",
       "          ...,\n",
       "          [ 55.6000, -16.6000,  18.7000,  ...,  16.6000,   7.6000,   2.2000],\n",
       "          [ 67.7000,  12.2000,  55.4000,  ...,  14.0000,  12.1000,  -5.4000],\n",
       "          [ 59.0000, -14.6000,  10.9000,  ...,  16.4000,   7.9000, -16.4000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 85.5000,  56.0000,   0.7000,  ...,  34.5000,  17.2000,  18.8000],\n",
       "          [ 85.0000,  53.0000, -11.7000,  ...,  22.2000,  14.7000,   6.4000],\n",
       "          [ 73.3000,  56.6000,  -7.5000,  ...,  13.3000,  15.3000,  11.9000],\n",
       "          ...,\n",
       "          [ 68.9000, -35.7000,  56.8000,  ...,  22.9000, -18.1000,  -4.1000],\n",
       "          [ 67.6000, -27.5000,  35.0000,  ...,  23.0000,  -4.3000,  -1.5000],\n",
       "          [ 70.6000, -21.2000,  60.5000,  ...,  22.1000,   5.2000,  23.5000]],\n",
       " \n",
       "         [[ 64.8000,  41.5000,  32.1000,  ...,  36.2000,  18.6000,  42.4000],\n",
       "          [ 90.2000,  69.3000,  60.3000,  ...,  51.6000,  41.8000,  50.4000],\n",
       "          [ 82.7000,  52.5000,  36.3000,  ...,  44.8000,  36.7000,  38.7000],\n",
       "          ...,\n",
       "          [ 74.7000,  13.4000, 127.7000,  ...,  38.5000,  18.7000,  42.4000],\n",
       "          [ 50.6000, -12.0000,  81.2000,  ...,  23.2000,  13.3000,  25.0000],\n",
       "          [ 69.3000,   9.7000, 115.1000,  ...,  34.9000,  17.0000,  30.7000]],\n",
       " \n",
       "         [[ 70.9000,  65.6000,  20.9000,  ...,  84.4000,  80.1000,  77.4000],\n",
       "          [ 78.4000,  79.2000,  22.5000,  ...,  91.1000,  85.9000,  85.8000],\n",
       "          [ 71.6000,  63.0000,  24.1000,  ...,  88.1000,  74.1000,  79.5000],\n",
       "          ...,\n",
       "          [ 80.3000,  34.5000, 105.6000,  ...,  85.6000, 100.9000,  85.8000],\n",
       "          [ 70.5000,  30.5000,  98.6000,  ...,  84.4000,  86.6000,  80.4000],\n",
       "          [ 83.1000,  29.6000, 111.1000,  ...,  91.1000,  87.2000,  88.5000]]]),\n",
       " 'labels': tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_path = \"../dataset/classification/FingerMovements/train.pt\"\n",
    "train_data = torch.load(train_dataset_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Dataset with samples + labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-9.5397e-01,  6.1665e-01, -3.7655e-01,  ...,  1.2540e-03,\n",
       "           8.8399e-01,  1.4750e-01],\n",
       "         [-8.3868e-01,  5.8382e-01, -2.3852e-01,  ...,  2.3525e-01,\n",
       "           6.7116e-01,  5.3996e-01],\n",
       "         [-7.5029e-01,  7.2689e-01,  2.5842e-01,  ...,  2.3525e-01,\n",
       "           8.6921e-01,  1.2874e+00],\n",
       "         ...,\n",
       "         [ 1.2205e-01, -2.6755e-01,  5.0412e-01,  ...,  1.0788e+00,\n",
       "           1.1589e+00, -3.2864e-01],\n",
       "         [ 2.2965e-01, -2.1287e-02,  8.5198e-01,  ...,  1.3159e+00,\n",
       "           1.3658e+00, -3.1999e-01],\n",
       "         [ 3.6416e-01, -3.9420e-01,  5.0136e-01,  ...,  1.2451e+00,\n",
       "           1.0229e+00, -3.0267e-01]]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset_Classification(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x = dataset[\"samples\"]  # (N, T, C)\n",
    "        self.y = dataset[\"labels\"]  # (N,)\n",
    "\n",
    "        # Calculate Means/stds:\n",
    "        self.means = self.x.mean(dim=[0, 1]) \n",
    "        self.stds = self.x.std(dim=[0, 1]) \n",
    "\n",
    "        # Normalize\n",
    "        self.x = (self.x - self.means) / self.stds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "train_dataset = Dataset_Classification(train_data)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up an Imputing Noising Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-9.5397e-01,  6.1665e-01, -3.7655e-01, -2.6380e-01,  1.1476e+00,\n",
       "          9.5103e-01,  5.6980e-01,  9.6900e-01,  1.0436e+00, -4.6374e-01,\n",
       "          3.5370e-01,  8.2001e-01,  9.7326e-01,  3.7221e-01, -3.8268e-01,\n",
       "          5.4273e-01,  8.8523e-01,  4.3582e-01,  7.7776e-02,  1.4664e+00,\n",
       "          1.2042e+00,  8.0010e-01,  8.1634e-01,  4.8460e-01,  5.7211e-01,\n",
       "          1.2540e-03,  8.8399e-01,  1.4750e-01]),\n",
       " tensor([ True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "          True,  True,  True, False, False,  True,  True, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask\n",
    "\n",
    "def noise_mask(X, masking_ratio, lm=3, mode='separate', distribution='geometric', exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n",
    "        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n",
    "            feat_dim that will be masked on average\n",
    "        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n",
    "        mode: whether each variable should be masked separately ('separate'), or all variables at a certain positions\n",
    "            should be masked concurrently ('concurrent')\n",
    "        distribution: whether each mask sequence element is sampled independently at random, or whether\n",
    "            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n",
    "            masked squences of a desired mean length `lm`\n",
    "        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n",
    "\n",
    "    Returns:\n",
    "        boolean numpy array with the same shape as X, with 0s at places where a feature should be masked\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        if mode == 'separate':  # each variable (feature) is independent\n",
    "            mask = np.ones(X.shape, dtype=bool)\n",
    "            for m in range(X.shape[1]):  # feature dimension\n",
    "                if exclude_feats is None or m not in exclude_feats:\n",
    "                    mask[:, m] = geom_noise_mask_single(X.shape[0], lm, masking_ratio)  # time dimension\n",
    "        else:  # replicate across feature dimension (mask all variables at the same positions concurrently)\n",
    "            mask = np.tile(np.expand_dims(geom_noise_mask_single(X.shape[0], lm, masking_ratio), 1), X.shape[1])\n",
    "    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n",
    "        if mode == 'separate':\n",
    "            mask = np.random.choice(np.array([True, False]), size=X.shape, replace=True,\n",
    "                                    p=(1 - masking_ratio, masking_ratio))\n",
    "        else:\n",
    "            mask = np.tile(np.random.choice(np.array([True, False]), size=(X.shape[0], 1), replace=True,\n",
    "                                            p=(1 - masking_ratio, masking_ratio)), X.shape[1])\n",
    "\n",
    "    return mask\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    \"\"\"Dynamically computes missingness (noise) mask for each sample\"\"\"\n",
    "\n",
    "    def __init__(self, data, mean_mask_length=3, masking_ratio=0.15,\n",
    "                 mode='separate', distribution='geometric', exclude_feats=None):\n",
    "        super(ImputationDataset, self).__init__()\n",
    "\n",
    "        # self.data = data  # this is a subclass of the BaseData class in data.py\n",
    "        self.data = data[\"samples\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "        # Normalise features: \n",
    "        self.means = self.data.mean(dim=[0, 1]) \n",
    "        self.stds = self.data.std(dim=[0, 1]) \n",
    "\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.means) / self.stds\n",
    "\n",
    "        \n",
    "\n",
    "        # self.feature_df = self.data.feature_df.loc[self.IDs]\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mean_mask_length = mean_mask_length\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "        self.exclude_feats = exclude_feats\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \"\"\"\n",
    "        For a given integer index, returns the corresponding (seq_length, feat_dim) array and a noise mask of same shape\n",
    "        Args:\n",
    "            ind: integer index of sample in dataset\n",
    "        Returns:\n",
    "            X: (seq_length, feat_dim) tensor of the multivariate time series corresponding to a sample\n",
    "            mask: (seq_length, feat_dim) boolean tensor: 0s mask and predict, 1s: unaffected input\n",
    "            ID: ID of sample\n",
    "        \"\"\"\n",
    "\n",
    "        # X = self.feature_df.loc[self.IDs[ind]].values  # (seq_length, feat_dim) array\n",
    "        X = self.data[ind].numpy()\n",
    "        label = self.labels[ind]\n",
    "\n",
    "        mask = noise_mask(X, self.masking_ratio, self.mean_mask_length, self.mode, self.distribution,\n",
    "                          self.exclude_feats)  # (seq_length, feat_dim) boolean array\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(mask), label\n",
    "\n",
    "    def update(self):\n",
    "        self.mean_mask_length = min(20, self.mean_mask_length + 1)\n",
    "        self.masking_ratio = min(1, self.masking_ratio + 0.05)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "imputation_dataset = ImputationDataset(train_data)\n",
    "sample, mask, label = imputation_dataset[0]\n",
    "sample[0], mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the collating function + DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6677,  1.2546,  0.0000,  ...,  0.3584,  0.0000,  0.0436],\n",
       "          [ 0.0000,  1.1842,  0.0000,  ...,  0.2876,  0.0000, -0.1007],\n",
       "          [ 0.0000,  1.3554,  0.5538,  ...,  0.2260,  0.3992, -0.1007],\n",
       "          ...,\n",
       "          [ 0.0144, -0.6780,  1.1474,  ..., -0.0000, -0.2038, -0.1440],\n",
       "          [ 0.3526, -0.2113,  1.6636,  ...,  0.0598, -0.1211, -0.1036],\n",
       "          [ 0.0529, -0.5724,  0.7885,  ...,  0.1090, -0.2156, -0.4729]],\n",
       " \n",
       "         [[ 0.0000,  0.3258,  0.5814,  ...,  1.2574,  0.1568,  0.0000],\n",
       "          [ 1.6554,  0.5228,  1.4207,  ...,  1.0111,  0.3726,  0.0000],\n",
       "          [ 1.8475,  0.6190,  0.7636,  ...,  1.1096,  0.3962,  0.0000],\n",
       "          ...,\n",
       "          [ 0.6370, -0.5982,  2.5112,  ...,  1.1866,  0.0000,  1.0103],\n",
       "          [ 0.6216, -1.1846,  0.0000,  ...,  1.0696,  0.0740,  1.0363],\n",
       "          [ 0.6293, -0.0000,  0.0000,  ...,  0.9495, -0.1536,  0.9901]],\n",
       " \n",
       "         [[-0.3391,  1.5759, -0.0000,  ..., -0.0000,  0.2632, -0.5912],\n",
       "          [ 0.4525,  1.9043, -0.6692,  ...,  0.0000,  0.5884, -0.2796],\n",
       "          [-0.1777,  1.2030, -0.0000,  ...,  0.0000,  0.0000, -0.6259],\n",
       "          ...,\n",
       "          [-0.2046,  0.0561,  0.8934,  ..., -0.5529,  0.0711, -0.1266],\n",
       "          [-0.0000, -0.5748, -0.3572,  ..., -0.5221, -0.3428, -0.6288],\n",
       "          [ 0.2297,  0.1663,  0.0000,  ..., -0.0603,  0.2780, -0.4210]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.7254,  0.6354, -0.0000,  ...,  0.5431,  0.1450,  0.0000],\n",
       "          [ 0.7062,  0.5651, -0.0000,  ...,  0.1644,  0.0711, -0.0000],\n",
       "          [ 0.2566,  0.6495, -0.0000,  ..., -0.1096,  0.0888,  0.1158],\n",
       "          ...,\n",
       "          [ 0.0875, -1.5153,  1.4290,  ...,  0.1860, -0.8985, -0.3460],\n",
       "          [ 0.0375, -1.3230,  0.8271,  ...,  0.1891, -0.4906, -0.2709],\n",
       "          [ 0.1528, -1.1752,  1.5311,  ...,  0.1614, -0.2097,  0.4505]],\n",
       " \n",
       "         [[ 0.0000,  1.1866,  0.0000,  ...,  0.0000,  0.9638,  1.3826],\n",
       "          [ 0.5870,  1.2523,  0.0000,  ...,  1.0881,  0.9490,  0.0000],\n",
       "          [ 0.2566,  0.9849,  0.0000,  ...,  1.2667,  1.1323,  1.0219],\n",
       "          ...,\n",
       "          [-0.5620,  0.5416,  2.5995,  ...,  0.1737,  1.6496,  1.0652],\n",
       "          [-0.8771, -0.2347,  1.8707,  ...,  0.3830,  1.3037,  1.1835],\n",
       "          [-0.3007,  0.3540,  2.4670,  ...,  0.3584,  1.2505,  0.9873]],\n",
       " \n",
       "         [[ 0.6293, -0.0213,  0.4682,  ...,  0.6724, -0.6590,  0.0000],\n",
       "          [ 0.0836, -0.4739,  0.0210,  ...,  0.2260, -0.9310,  0.0000],\n",
       "          [ 0.3834,  0.2578,  0.3799,  ...,  0.1521, -0.0117,  0.0000],\n",
       "          ...,\n",
       "          [-1.1999, -0.0000,  1.2468,  ..., -0.0141, -0.0235, -0.2652],\n",
       "          [-0.8272, -0.0000,  1.9480,  ..., -0.0295, -0.1743,  0.0840],\n",
       "          [-1.5611, -0.0000,  1.3627,  ..., -0.4390, -0.7389,  0.0234]]]),\n",
       " tensor([[[ 0.6677,  1.2546,  0.5152,  ...,  0.3584,  0.3549,  0.0436],\n",
       "          [ 0.1528,  1.1842,  0.1563,  ...,  0.2876,  0.3608, -0.1007],\n",
       "          [ 0.5601,  1.3554,  0.5538,  ...,  0.2260,  0.3992, -0.1007],\n",
       "          ...,\n",
       "          [ 0.0144, -0.6780,  1.1474,  ..., -0.1003, -0.2038, -0.1440],\n",
       "          [ 0.3526, -0.2113,  1.6636,  ...,  0.0598, -0.1211, -0.1036],\n",
       "          [ 0.0529, -0.5724,  0.7885,  ...,  0.1090, -0.2156, -0.4729]],\n",
       " \n",
       "         [[ 2.0358,  0.3258,  0.5814,  ...,  1.2574,  0.1568,  1.0709],\n",
       "          [ 1.6554,  0.5228,  1.4207,  ...,  1.0111,  0.3726,  1.1950],\n",
       "          [ 1.8475,  0.6190,  0.7636,  ...,  1.1096,  0.3962,  1.1719],\n",
       "          ...,\n",
       "          [ 0.6370, -0.5982,  2.5112,  ...,  1.1866,  0.1864,  1.0103],\n",
       "          [ 0.6216, -1.1846,  1.8873,  ...,  1.0696,  0.0740,  1.0363],\n",
       "          [ 0.6293, -0.8046,  2.5333,  ...,  0.9495, -0.1536,  0.9901]],\n",
       " \n",
       "         [[-0.3391,  1.5759, -1.3594,  ..., -0.5283,  0.2632, -0.5912],\n",
       "          [ 0.4525,  1.9043, -0.6692,  ...,  0.1767,  0.5884, -0.2796],\n",
       "          [-0.1777,  1.2030, -1.3263,  ...,  0.1644,  0.1598, -0.6259],\n",
       "          ...,\n",
       "          [-0.2046,  0.0561,  0.8934,  ..., -0.5529,  0.0711, -0.1266],\n",
       "          [-0.7157, -0.5748, -0.3572,  ..., -0.5221, -0.3428, -0.6288],\n",
       "          [ 0.2297,  0.1663,  0.7498,  ..., -0.0603,  0.2780, -0.4210]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.7254,  0.6354, -0.1198,  ...,  0.5431,  0.1450,  0.3149],\n",
       "          [ 0.7062,  0.5651, -0.4621,  ...,  0.1644,  0.0711, -0.0430],\n",
       "          [ 0.2566,  0.6495, -0.3462,  ..., -0.1096,  0.0888,  0.1158],\n",
       "          ...,\n",
       "          [ 0.0875, -1.5153,  1.4290,  ...,  0.1860, -0.8985, -0.3460],\n",
       "          [ 0.0375, -1.3230,  0.8271,  ...,  0.1891, -0.4906, -0.2709],\n",
       "          [ 0.1528, -1.1752,  1.5311,  ...,  0.1614, -0.2097,  0.4505]],\n",
       " \n",
       "         [[ 0.1374,  1.1866,  0.1756,  ...,  0.8602,  0.9638,  1.3826],\n",
       "          [ 0.5870,  1.2523,  0.3937,  ...,  1.0881,  0.9490,  0.7708],\n",
       "          [ 0.2566,  0.9849,  0.2750,  ...,  1.2667,  1.1323,  1.0219],\n",
       "          ...,\n",
       "          [-0.5620,  0.5416,  2.5995,  ...,  0.1737,  1.6496,  1.0652],\n",
       "          [-0.8771, -0.2347,  1.8707,  ...,  0.3830,  1.3037,  1.1835],\n",
       "          [-0.3007,  0.3540,  2.4670,  ...,  0.3584,  1.2505,  0.9873]],\n",
       " \n",
       "         [[ 0.6293, -0.0213,  0.4682,  ...,  0.6724, -0.6590,  0.4014],\n",
       "          [ 0.0836, -0.4739,  0.0210,  ...,  0.2260, -0.9310,  0.0754],\n",
       "          [ 0.3834,  0.2578,  0.3799,  ...,  0.1521, -0.0117,  0.2774],\n",
       "          ...,\n",
       "          [-1.1999, -0.3801,  1.2468,  ..., -0.0141, -0.0235, -0.2652],\n",
       "          [-0.8272, -0.4012,  1.9480,  ..., -0.0295, -0.1743,  0.0840],\n",
       "          [-1.5611, -0.9665,  1.3627,  ..., -0.4390, -0.7389,  0.0234]]]),\n",
       " tensor([[[False, False,  True,  ..., False,  True, False],\n",
       "          [ True, False,  True,  ..., False,  True, False],\n",
       "          [ True, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[ True, False, False,  ..., False, False,  True],\n",
       "          [False, False, False,  ..., False, False,  True],\n",
       "          [False, False, False,  ..., False, False,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False,  True, False],\n",
       "          [False, False,  True,  ..., False, False, False],\n",
       "          [False,  True,  True,  ..., False, False, False]],\n",
       " \n",
       "         [[False, False,  True,  ...,  True, False, False],\n",
       "          [False, False, False,  ...,  True, False, False],\n",
       "          [False, False,  True,  ...,  True,  True, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [ True, False, False,  ..., False, False, False],\n",
       "          [False, False,  True,  ..., False, False, False]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[False, False,  True,  ..., False, False,  True],\n",
       "          [False, False,  True,  ..., False, False,  True],\n",
       "          [False, False,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[ True, False,  True,  ...,  True, False, False],\n",
       "          [False, False,  True,  ..., False, False,  True],\n",
       "          [False, False,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False,  True],\n",
       "          [False, False, False,  ..., False, False,  True],\n",
       "          [False, False, False,  ..., False, False,  True],\n",
       "          ...,\n",
       "          [False,  True, False,  ..., False, False, False],\n",
       "          [False,  True, False,  ..., False, False, False],\n",
       "          [False,  True, False,  ..., False, False, False]]]),\n",
       " tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]),\n",
       " (tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(1.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(1.)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compensate_masking(X, mask):\n",
    "    \"\"\"\n",
    "    Compensate feature vectors after masking values, in a way that the matrix product W @ X would not be affected on average.\n",
    "    If p is the proportion of unmasked (active) elements, X' = X / p = X * feat_dim/num_active\n",
    "    Args:\n",
    "        X: (batch_size, seq_length, feat_dim) torch tensor\n",
    "        mask: (batch_size, seq_length, feat_dim) torch tensor: 0s means mask and predict, 1s: unaffected (active) input\n",
    "    Returns:\n",
    "        (batch_size, seq_length, feat_dim) compensated features\n",
    "    \"\"\"\n",
    "\n",
    "    # number of unmasked elements of feature vector for each time step\n",
    "    num_active = torch.sum(mask, dim=-1).unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "    # to avoid division by 0, set the minimum to 1\n",
    "    num_active = torch.max(num_active, torch.ones(num_active.shape, dtype=torch.int16))  # (batch_size, seq_length, 1)\n",
    "    return X.shape[-1] * X / num_active\n",
    "\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val()  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len, device=lengths.device)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))\n",
    "\n",
    "\n",
    "def collate_unsuperv(data, max_len=None, mask_compensation=False):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, mask).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - mask: boolean torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 ignore (padding)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, masks, labels = zip(*data)\n",
    "\n",
    "    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n",
    "    lengths = [X.shape[0] for X in features]  # original sequence length for each time series\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    target_masks = torch.zeros_like(X,\n",
    "                                    dtype=torch.bool)  # (batch_size, padded_length, feat_dim) masks related to objective\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "        target_masks[i, :end, :] = masks[i][:end, :]\n",
    "\n",
    "    targets = X.clone()\n",
    "    X = X * target_masks  # mask input\n",
    "    if mask_compensation:\n",
    "        X = compensate_masking(X, target_masks)\n",
    "\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16), max_len=max_len)  # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "    target_masks = ~target_masks  # inverse logic: 0 now means ignore, 1 means predict\n",
    "    return X, targets, target_masks, padding_masks, labels\n",
    "\n",
    "\n",
    "\n",
    "# Setting up the dataloader: \n",
    "batch_size = 32\n",
    "max_seq_len = 50\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=imputation_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda x : collate_unsuperv(x, max_len=max_seq_len)\n",
    ")\n",
    "\n",
    "\n",
    "next(iter(train_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This should work. Dataloader returns, Inputs, targets, targets_mask, padding_mask + Labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
